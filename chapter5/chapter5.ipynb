{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络CNN(conventional neural network)\n",
    "在图像处理中用的尤其多的一种方法,每一层神经元都进行卷积操作,使得图片的维度减小,深度增加."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weights sharing\n",
    "如果两个特征拥有相同的信息, 他们可以使用同一个权重  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相同填充和有效填充padding\n",
    "相同填充是指在原始图像周围加0来构造patch,有效填充是指直接在图像上平移来构造patch  \n",
    "值得注意的另一点是神经网络的深度,每一个patch是\"一层\"神经网络,但是不是指深度是1. 每一个patch的depth都和上一层数据的depth一样.  \n",
    "而有多少个patch,下一层的depth就是多少. \n",
    "\n",
    "SAME Padding， 输出的高和宽，计算如下：\n",
    "out_height = ceil(float(in_height) / float(strides1))\n",
    "\n",
    "VALID Padding， 输出的高和宽，计算如下：\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense层 全连接层   \n",
    "链式法则没有任何变化  \n",
    "输入层（input layer）维度值为$W$， 滤波器（filter）的维度值为 F $(height * width * depth)$， stride 的数值为 $S$， padding 的数值为 $P$， 下一层的维度值可用如下公式表示: $(W−F+2P)/S+1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在,参数共享的作用体现出来了,原本有$(W+1) \\times F$个参数,现在只有$(W+1)\\times k$  \n",
    "+1是加了偏置,而k是神经元的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层的构建\n",
    "```\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "```  \n",
    "TensorFlow 对每一个 input 维度使用一个单独的 stride 参数，[batch, input_height, input_width, input_channels]。我们通常把 batch 和 input_channels （strides 序列中的第一个第四个）的 stride 设为 1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 滤波器的选择\n",
    "F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) # (height, width, input_depth, output_depth)  \n",
    "F_b = tf.Variable(tf.zeros(3)) # (output_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化pooling\n",
    "~~从数据层输出到卷积层,即每一个patch怎么计算叫做池化~~  \n",
    "池化是一种减小输出数据量,和抵抗过拟合的做法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大池化:在池化滤波器中取那个最大的值\n",
    "```\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "```  \n",
    "ksize 和 strides 参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 ([batch, height, width, channels])，对 ksize 和 strides 来说，batch 和 channel 通常都设置成 1。  \n",
    "### 平均池化:取平均值\n",
    "### $1\\times1$卷积\n",
    "### inception方法\n",
    "我们可以对每一层进行池化,$1\\times1$卷积,$3\\times3$卷积,$5\\times5$卷积,inception就是全都用上,然后叠在深度空间里,让接下来的的神经网络来做筛选,因为其中可以有很多共享参数,所以是一种低耗高效的方法  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
