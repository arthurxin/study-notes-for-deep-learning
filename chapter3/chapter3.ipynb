{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 Tensorflow 入门\n",
    "## 安装\n",
    "### mac/linux\n",
    "conda create -n tensorflow python=3.5  \n",
    "source activate tensorflow  \n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn  \n",
    "conda install -c conda-forge tensorflow  \n",
    "### windows \n",
    "conda create -n tensorflow python=3.5  \n",
    "activate tensorflow  \n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn  \n",
    "conda install -c conda-forge tensorflow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow基本操作\n",
    "定义工作空间 \n",
    "```\n",
    "with tf.Session() as sess:  \n",
    "    output = sess.run(hello_constant)  \n",
    "```\n",
    "定义常量 \n",
    "```\n",
    "x=tf.constant(13)  \n",
    "```\n",
    "\n",
    "定义数据类型(在session之前做)\n",
    "```\n",
    "x=tf.placeholder(int32)  \n",
    "```\n",
    "赋值   \n",
    "```\n",
    "sess.run(x,feed_dict={x:13})   \n",
    "```\n",
    "\n",
    "加法 减法 乘法 除法\n",
    "```\n",
    "x = tf.add(5, 2)  # 7  \n",
    "x = tf.subtract(10, 4) # 6  \n",
    "y = tf.multiply(2, 5)  # 10  \n",
    "z = tf.divide(9,3)  # 3 #除法计算之后数据是float64\n",
    "```\n",
    "数据类型转换\n",
    "```\n",
    "tf.cast(tf.constant(2.0), tf.int32  \n",
    "```\n",
    "\n",
    "变量 \n",
    "```\n",
    "tf.Variable() \n",
    "```\n",
    "需要初始化(在session之前做) \n",
    "```\n",
    "init_variables_initializer()  \n",
    "```\n",
    "随机取样一个tensor\n",
    "```\n",
    "tf.truncated_normal() \n",
    "例子:x = tf.Variable(tf,truncated_normal((10,5))) \n",
    "```\n",
    "注:构建模型的时候常常只对权重初始随机,偏置直接初始为0  \n",
    "取零构建tensor \n",
    "```\n",
    "tf.zeros()  \n",
    "```\n",
    "\n",
    "softmax已经写好 \n",
    "```\n",
    "x = tf.nn.softmax() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 独热编码\n",
    "使用skit-learn中的LabelBinarizer来把非数值变量转化为可以放入模型中计算的变量  \n",
    "```\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Example labels 示例 labels\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# Create the encoder 创建编码器\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Here the encoder finds the classes and assigns one-hot vectors \n",
    "# 编码器找到类别并分配 one-hot 向量\n",
    "lb.fit(labels)\n",
    "\n",
    "# And finally, transform the labels into one-hot encoded vectors\n",
    "# 最后把目标（lables）转换成独热编码的（one-hot encoded）向量\n",
    "lb.transform(labels)\n",
    ">>> array([[1, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 1],\n",
    "           [0, 0, 1, 0, 0],\n",
    "           [0, 1, 0, 0, 0],\n",
    "           [1, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 1, 0],\n",
    "           [0, 1, 0, 0, 0],\n",
    "           [1, 0, 0, 0, 0],\n",
    "           [0, 0, 1, 0, 0]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow高级用法\n",
    "数组求和\n",
    "```\n",
    "tf.reduce_sum()\n",
    "```\n",
    "求对数\n",
    "```\n",
    "tf.log()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵\n",
    "对于tensor的交叉熵,定义为 $ D(\\, \\widehat{y},y \\,) = - \\sum_{j} y_j \\, ln \\widehat{y_j} $  \n",
    "在softmax进行图像分类的时候,输出是一个向量 $\\widehat{y}$ (n分类就是长度为n的向量),给出的分类结果 $y$ 是一个长度为$n$的独热向量  \n",
    "在公式中的乘法是元素相乘,所以用tf.multiply而不是tf.matmul ~~,这与之前预测数值时使用的是一致的~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化的重要性\n",
    "归一化可以避免Python在计算极大数和极小数时出现未知的错误  \n",
    "初始化权重时,建议选择方差叫小的正态分布  \n",
    "为什么较小的方差意味着不那么自信的估计?(待补充)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集,测试集,验证集的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降SGD\n",
    "原理  \n",
    "从数据集中随机抽样出batch,只用batch不用整个数据集来算权重和偏置,指数级的减小运算量.但是因此计算的时候会没那么准确,需要一些技巧来弥补.\n",
    "技巧 \n",
    "- 初始时的权重平均值为0,方差较小  \n",
    "- 使得学习速率随着学习的进行不断减小(learning rate decy)  \n",
    "- 计算梯度的时候把前几次的梯度与这一次的求平均,有效减少随机噪音(momentum)  \n",
    "优点:可调超参量多\n",
    "- 初始化权重\n",
    "- 初始学习速率\n",
    "- 学习速率衰减\n",
    "- 动量计算使用数据数\n",
    "- batch size\n",
    "- epochs 这种粗糙的使用方法使得数据的利用率下降,为了尽可能的利用珍贵的数据,我们对数据进行多次使用\n",
    "简介   adagrad是一种自动选择超参数的算法,不一定最准,只适合小模型\n",
    "每一个batch算出来的权重怎么结合在一起?(待补充) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
